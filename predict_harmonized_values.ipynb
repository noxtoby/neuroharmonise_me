{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harmonizing a test set of FreeSurfer 7.1.1 volumes using NeuroHarmony (COMBAT + MRI-QC)\n",
    "\n",
    "## E-DADS\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Load FS and MRIQC outputs\n",
    "2. Wrangle to match `dummy_test.csv` header and divide volumes by `ICV`/`eTIV`\n",
    "3. Load trained model\n",
    "4. Run it\n",
    "\n",
    "### Observations\n",
    "\n",
    "- Does it work only for cross-sectional test data?\n",
    "\n",
    "### Authors\n",
    "\n",
    "- Neil Oxtoby, UCL: step 2 (wrangling)\n",
    "- Vikram Venkatraghavan, AUMC (everything else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from neuroharmony import Neuroharmony\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDIT ME: Paths to data, CSV filenames, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE ME\n",
    "edads_folder = Path.home() / \"Documents\" / \"research-projects\" / \"E-DADS_MR-T046422-1\"\n",
    "top_folder = edads_folder / \"projects\" / \"2023-05_Harmonization_Paper\" / \"20231023-Model_For_Harmonizing_AIBL\"\n",
    "\n",
    "csv_aibl_demographics = top_folder / 'AIBLMERGE_freesurfer7p1p1-20220915.csv'\n",
    "csv_test_freesurfer7p1p1 = top_folder / 'aibl_freesurfer7p1p1_frompierrick.csv'\n",
    "tsv_test_mriqc0p16p1     = top_folder / 'aibl_mriqc_from_pierrick.tsv'\n",
    "\n",
    "csv_test_wrangled   = top_folder / 'aibl_test_for_neuroharmony.csv'\n",
    "csv_test_harmonised = Path(str(csv_test_wrangled).replace('.csv','-harmonised.csv'))\n",
    "\n",
    "str_pickle = top_folder / 'pickle_files'\n",
    "\n",
    "if 'noxtoby' in str(Path().home()):\n",
    "    # Neil's path\n",
    "    str_pickle = Path.home() / \"Library/CloudStorage/OneDrive-UniversityCollegeLondon/Data-from-Neil-Oxtoby/E-DADS_harmonization/raw\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy CSV for correct formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dummy_test = top_folder / 'dummy_test.csv'\n",
    "df_dummy_test = pd.read_csv(csv_dummy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECK ME: Load data\n",
    "\n",
    "- Columns may need renaming\n",
    "- Coding may need checking:\n",
    "  - `Sex`: {0:'Male',1:'Female'}\n",
    "  - `Diagnosis`: 0 unimpaired; 1 impaired\n",
    "  - `scanner`: TBD\n",
    "  - `cohort`: Just give it a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = ['ID','visit'] # for merging tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_mriqc = pd.read_csv(tsv_test_mriqc0p16p1,sep='\\t')\n",
    "\n",
    "df_test_fs    = pd.read_csv(csv_test_freesurfer7p1p1)\n",
    "# Rename columns\n",
    "df_test_fs.rename(columns={'rid':'ID'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aibl_demog = pd.read_csv(csv_aibl_demographics)\n",
    "# Rename column(s)\n",
    "df_aibl_demog.rename(columns={'TP':'visit'},inplace=True)\n",
    "# Recode\n",
    "df_aibl_demog['Sex_text'] = df_aibl_demog['Sex'].values\n",
    "df_aibl_demog['Sex'] = df_aibl_demog['Sex'].map({\"M\":0,\"F\":1})\n",
    "df_aibl_demog['Diagnosis'] = df_aibl_demog['Diagnostic Criteria'].map({'CN':0,'MCI':1,'AD':1})\n",
    "\n",
    "# Add `cohort` column\n",
    "df_aibl_demog['cohort'] = 'AIBL-LONI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a quick squiz at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aibl_demog.drop_duplicates(subset=key,inplace=True) # Why are there always duplicates?\n",
    "df_aibl_demog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_fs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_mriqc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDIT ME: Data wrangling\n",
    "\n",
    "This is likely to be specific to your CSVs. What I did:\n",
    "\n",
    "- MRIQC\n",
    "  - Added \"ID\" and \"visit\", based on the available BIDS name (sub/ses). For joining with FreeSurfer.\n",
    "- FreeSurfer\n",
    "  - Removed duplicates (subset=['ID','visit']), preferring those with fewer `SurfaceHoles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRIQC\n",
    "print(\"Wrangling MRIQC\\n\")\n",
    "\n",
    "# Add ID \n",
    "df_test_mriqc['ID'] = df_test_mriqc['bids_name'].map(lambda x: x.split('_ses')[0].split('sub-')[-1]).astype(int)\n",
    "# Add visit\n",
    "df_test_mriqc['visit'] = df_test_mriqc['bids_name'].map(lambda x: x.split('_ses-')[-1].split('_T1')[0])\n",
    "# Reorder columns\n",
    "df_test_mriqc = df_test_mriqc[key + [c for c in df_test_mriqc.columns.tolist() if c not in key]]\n",
    "df_test_mriqc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FreeSurfer\n",
    "print(\"Identifying duplicates in FreeSurfer CSV\\n\")\n",
    "\n",
    "print(f\"Not sure why there are {key} duplicates in FreeSurfer, but they tend to differ by SurfaceHoles so perhaps that's useful:\\n\")\n",
    "idx = list(df_test_fs.drop_duplicates(subset=key).index.values)\n",
    "list(df_test_fs.index.values)\n",
    "idx_having_duplicates = [ i for i in list(df_test_fs.index.values) if i not in idx ]\n",
    "df_test_fs.loc[df_test_fs['ID'].isin(df_test_fs.loc[idx_having_duplicates,'ID'].tolist())].sort_values(by=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FreeSurfer\n",
    "print(\"Wrangling FreeSurfer\\n\")\n",
    "\n",
    "print(f\"Before: df.shape: {df_test_fs.shape}\")\n",
    "# Drop duplicates\n",
    "df_test_fs = df_test_fs.sort_values(by=key+['SurfaceHoles']).drop_duplicates(subset=key).reset_index(drop=True)\n",
    "print(f\"After:  df.shape: {df_test_fs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge FreeSurfer and MRIQC\n",
    "\n",
    "Then match columns with `dummy_test.csv`\n",
    "\n",
    "- Extract missing ones from wherever I can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.merge(df_test_fs,df_test_mriqc,on=key,how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"df_test_fs.shape   : {df_test_fs.shape}\")\n",
    "# print(f\"df_test_fs.drop_duplicates().shape   : {df_test_fs.drop_duplicates(subset=key).shape}\")\n",
    "# print(\"\")\n",
    "# print(f\"df_test_mriqc.shape: {df_test_mriqc.shape}\")\n",
    "# print(f\"df_test_mriqc.drop_duplicates().shape   : {df_test_mriqc.drop_duplicates(subset=key+['bids_name']).shape}\")\n",
    "# print(\"\")\n",
    "# print(f\"df_test.shape      : {df_test.shape}\")\n",
    "# print(f\"df_test.drop_duplicates().shape      : {df_test.drop_duplicates(subset=key).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match test data columns to dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonise the columns\n",
    "in_both_dummy_and_test = [c for c in df_test.columns.tolist() if c in df_dummy_test.columns.tolist()]\n",
    "in_test_and_not_dummy  = [c for c in df_test.columns.tolist() if c not in df_dummy_test.columns.tolist()]\n",
    "in_dummy_and_not_test  = [c for c in df_dummy_test.columns.tolist() if c not in df_test.columns.tolist()]\n",
    "\n",
    "# print(f'In both dummy and real:\\n{\", \".join(in_both_dummy_and_test)}\\n')\n",
    "# print(\"\")\n",
    "\n",
    "print(f'In dummy but not real (add these from AIBL):\\n{\", \".join(in_dummy_and_not_test)}\\n')\n",
    "\n",
    "# print(\"\")\n",
    "# print(f'In real  but not dummy:\\n{\", \".join(in_test_and_not_dummy)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_demog = ['cohort','Age','Sex','Diagnosis']\n",
    "df_aibl_demog[in_demog]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Demographics merge.\\nBefore: df.shape: {df_test.shape}\")\n",
    "df_test = pd.merge(df_test,df_aibl_demog[key + in_demog],on=key,how='inner')\n",
    "print(f\"After:  df.shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIXME: get `scanner` information from somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['scanner'] = 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colz = df_dummy_test.columns.tolist()\n",
    "df_test = df_test[colz].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling: dividing test volumes by TIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vols = [c for c in df_test.columns.tolist() if ( 'vol' in str.lower(c) ) | ( 'left' in str.lower(c) ) | ( 'right' in str.lower(c) ) | (c=='CSF')]\n",
    "tiv_col = 'eTIV'\n",
    "for v in vols:\n",
    "    df_test[v] = df_test[v]/df_test[tiv_col]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(csv_test_wrangled,index=False)\n",
    "print(f\"Wrangled test data written out to {csv_test_wrangled.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<hr/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the wrangled test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = pd.read_csv(csv_test_wrangled, low_memory=False)\n",
    "# Xtest needs to be in the format mentioned in dummy_test.csv \n",
    "# Note: The volumes should be expressed as fractions of eTIV\n",
    "# Sex --> should be 0 / 1 {0 for male, 1 for female}\n",
    "# Diagnosis --> should be 0 / 1 {0 for unimpaired cognition, 1 otherwise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Vikram's code from here\n",
    "\n",
    "Contains some magic parameters based on the prescribed format for TEST data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = list(Xtest)[2:88]\n",
    "regression_features = list(Xtest)[88:157]\n",
    "\n",
    "# Neil code\n",
    "testing_on_single_roi = True \n",
    "if testing_on_single_roi:\n",
    "    #rois = [rois[0]]\n",
    "    rois = ['Left-Thalamus']\n",
    "# END Neil code\n",
    "\n",
    "covariates = [\"Sex\", \"scanner\", 'Age','Diagnosis']\n",
    "eliminate_variance = [\"scanner\"]\n",
    "discrete_cols = ['Sex','Diagnosis']\n",
    "continuous_cols = ['Age']\n",
    "\n",
    "Xtest_prepared = Xtest[['ID','cohort']+rois+regression_features+covariates].copy(deep=True)\n",
    "Xharmonized = Xtest_prepared.copy(deep=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for r in tqdm(rois):\n",
    "    model = Neuroharmony(\n",
    "        [r],\n",
    "        regression_features,\n",
    "        covariates,\n",
    "        eliminate_variance,\n",
    "        discrete_cols = discrete_cols,\n",
    "        continuous_cols = continuous_cols,\n",
    "        param_distributions=dict(\n",
    "            RandomForestRegressor__n_estimators=[100, 200, 500],\n",
    "            RandomForestRegressor__random_state=[42, 78],\n",
    "            RandomForestRegressor__warm_start=[False, True],\n",
    "        ),\n",
    "        estimator_args=dict(n_jobs=64, random_state=42),\n",
    "        randomized_search_args=dict(cv=5,n_jobs=64)\n",
    "    )\n",
    "\n",
    "    pickle_name = str_pickle / (r + '.pickle') # Edited by Neil\n",
    "\n",
    "    # Added by Neil\n",
    "    if not pickle_name.exists():\n",
    "        print(f\"{pickle_name.name} file not found. Looking for zip file.\")\n",
    "        pickle_name_zip = Path(str(pickle_name) + '.gz')\n",
    "        if pickle_name_zip.exists():\n",
    "            raise NotImplementedError(f\"{pickle_name_zip.name} zip-file found. Extracting (FIXME: not implemented).\")\n",
    "        else:\n",
    "            print(f\"{pickle_name_zip.name} zip-file not found.\")\n",
    "\n",
    "    [model_roi,coverage_roi] = pickle.load(open(pickle_name,'rb'))\n",
    "\n",
    "    model.models_by_feature_ = dict.fromkeys([r],model_roi)\n",
    "    model.coverage_ = coverage_roi\n",
    "    model.extra_vars = regression_features+covariates\n",
    "    Xp=model.predict(Xtest_prepared)\n",
    "    Xharmonized[r] = Xp[r].copy(deep=True)\n",
    "    del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing_on_single_roi:\n",
    "    from matplotlib import pyplot as plt\n",
    "    #import seaborn\n",
    "    fig,ax=plt.subplots(1,2,figsize=(12,5))\n",
    "    ax[0].boxplot([Xtest_prepared[r],Xharmonized[r]])\n",
    "    ax[0].set_xticks([1,2],labels=['Pre','Post'])\n",
    "    ax[1].plot(Xtest_prepared[r],Xharmonized[r],'x')\n",
    "    ax[1].set_xlabel('Pre')\n",
    "    ax[1].set_ylabel('Post')\n",
    "    fig.suptitle(r)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xharmonized.to_csv(csv_test_harmonised, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmonization_v2_neil",
   "language": "python",
   "name": "harmonization_v2_neil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
